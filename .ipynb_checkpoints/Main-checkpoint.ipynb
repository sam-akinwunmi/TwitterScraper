{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Twitter Moods to predict the FTSE 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to determine whether the measurements of collective mood states derived from Twitter feeds are correlated to, and predictive of, the value of the FTSE 100 index over time. I will be collecting a daily grouping of a sample of tweets from the UK along with the sentiment and mood of these tweets, and the daily closing price of the FTSE 100 index.\n",
    "\n",
    "After analysing the Tweets, I will be using a Neural Network to predict the future value of the FTSE 100 index based on the Twitter mood, measuring the mean absolute percentage error (MAPE); the absolute percentage difference between the actual value and the predicted forecast.\n",
    "\n",
    "My hypothesis that twitter will be in some way predictive of movements in the FTSE 100 index.\n",
    "\n",
    "This piece of work is inspired by Johan Bollen et al. Their paper can be found here - https://arxiv.org/pdf/1010.3003.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting a years worth of Tweets from Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was originally going to use the Twitter API to collect all the tweets from Twitter, but there are various restrictions whilst using the API. You can only get historical tweets going back 7 days, and there is a limit to how many requests you can make every 15 mins.\n",
    "\n",
    "I looked into using beautifulsoup to scrape the search data from Twitter, but Ahmet Taspinar - http://ataspinar.com/ - has already created a library which scrapes data from Twitter, and returns the results as a json. The library is called twitter scraper. You can install the library directly using pip as shown below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Command Line\n",
    "C:\\...> pip install twitterscraper\n",
    "\n",
    "# Test that twitterscraper is working\n",
    "C:\\...> twitterscraper Trump --limit 20 --output=test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That will save the top 20 tweets with the word \"Trump\" to a file called 'test.json' which looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"fullname\": \"Betsy Holahan\", \n",
      "        \"id\": \"857296620395909120\", \n",
      "        \"text\": \"White House unveils dramatic plan to overhaul tax code in major test for Trump http://wapo.st/2q8phNF?tid=ss_tw\\u00a0\\u2026\", \n",
      "        \"timestamp\": \"2017-04-26T18:13:36\", \n",
      "        \"user\": \"GreatPointStrat\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# PrettyPrint a json file\n",
    "def PrettyPrint(file, x=None):\n",
    "    with open(file) as open_file:\n",
    "        file_parsed = json.load(open_file)\n",
    "        print json.dumps(file_parsed[:x], indent=4, sort_keys=True)\n",
    "\n",
    "# Only show 1 example for this demo\n",
    "PrettyPrint('test.json', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tested twitterscraper, we can extend the command to include our search query and date range. We type the commands below into different instances of the terminal, and wait for our data to be collected."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Command Line\n",
    "C:\\...> twitterscraper %22i%20feel%22%20since%3A2016-04-27%20until%3A2017-04-27 --output=ifeel.json\n",
    "C:\\...> twitterscraper %22i%20am%20feeling%22%20since%3A2016-04-27%20until%3A2017-04-27 --output=ifeel.json\n",
    "C:\\...> twitterscraper %22i'm%20feeling%22%20since%3A2016-04-27%20until%3A2017-04-27 --output=ifeel.json\n",
    "C:\\...> twitterscraper %22i%20dont%20feel%22%20since%3A2016-04-27%20until%3A2017-04-27 --output=ifeel.json\n",
    "C:\\...> twitterscraper %22makes%20me%22%20since%3A2016-04-27%20until%3A2017-04-27 --output=ifeel.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the 1st command for 12 hours, I'd only managed to collect 1.1 million tweets which only went back 2 weeks...\n",
    "\n",
    "We can try to use twitterscraper directly from within Python instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from twitterscraper import query_tweets\n",
    "\n",
    "start_date = date(2016, 4, 28)\n",
    "mid_date = date(2016, 4, 29)\n",
    "end_date = date(2017, 4, 28\n",
    "\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "while mid_date <= end_date:\n",
    "    since = start_date.strftime(\"%Y-%m-%d\")\n",
    "    until = mid_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    test_tweets = query_tweets(\"%22i%20feel%22%20since%3A\"+since+\"%20until%3A\"+until)\n",
    "    \n",
    "    time_file = open('timestamp.txt', 'a')\n",
    "    text_file = open('tweets.txt', 'a')\n",
    "    \n",
    "    for tweet in test_tweets:\n",
    "        i += 1\n",
    "        # collect every 1000th tweet to speed it up\n",
    "        if i % 1000 == 0:\n",
    "            time_file.write('%s \\n' % tweet.timestamp)\n",
    "            text_file.write('%s \\n' % tweet.text.encode('utf-8').strip().replace('\\n',' ').replace('\\t',' '))\n",
    "    \n",
    "    start_date += delta\n",
    "    mid_date += delta\n",
    "    \n",
    "    time_file.close()\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I collected all of the data from the 18th Apr 2017 to the most recent date, but for some reason, I struggled to get anything prior to that date. It just keeps failing..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
